{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98adaebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62931991",
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from selenium.common.exceptions import NoSuchElementException,StaleElementReferenceException  #importing exception\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47318006",
   "metadata": {},
   "source": [
    "1. Write a python program which searches all the product under a particular product from www.amazon.in. The\n",
    "product to be searched will be taken as input from user. For e.g. If user input is ‘guitar’. Then search for\n",
    "guitars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a019567a",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome()\n",
    "driver.get(\"https://www.amazon.in.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e883a322",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input=input('Enter the product name ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "011048ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70eefe35",
   "metadata": {},
   "outputs": [],
   "source": [
    "product_search = driver.find_element(By.XPATH,\"/html/body/div[1]/header/div/div[1]/div[2]/div/form/div[2]/div[1]/input\")\n",
    "product_search.send_keys(user_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb6278a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "search = driver.find_element(By.XPATH,\"/html/body/div[1]/header/div/div[1]/div[2]/div/form/div[3]/div/span/input\")\n",
    "search.click()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa6169c",
   "metadata": {},
   "source": [
    "2. In the above question, now scrape the following details of each product listed in first 3 pages of your search\n",
    "results and save it in a data frame and csv. In case if any product has less than 3 pages in search results then\n",
    "scrape all the products available under that product name. Details to be scraped are: \"Brand\n",
    "Name\", \"Name of the Product\", \"Price\", \"Return/Exchange\", \"Expected Delivery\", \"Availability\" and\n",
    "“Product URL”. In case, if any of the details are missing for any of the product then replace it by “-“. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaeb78c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_url=[]\n",
    "for i in range(0,1):\n",
    "    url= driver.find_elements(By.XPATH,'//a[@class=\"a-link-normal s-underline-text s-underline-link-text s-link-style a-text-normal\"]')\n",
    "    for i in url:\n",
    "        all_url.append(i.get_attribute('href'))\n",
    "        next_button= driver.find_element(By.XPATH,'/html/body/div[1]/div[1]/div[1]/div[1]/div/span[1]/div[1]/div[64]/div/div/span/a[3]')\n",
    "        #next_button.click()\n",
    "    time.sleep(10)\n",
    "        \n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "len(all_url)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ebfd63",
   "metadata": {},
   "outputs": [],
   "source": [
    "brand_name = []\n",
    "product_name = []\n",
    "prices = []\n",
    "return_exchange = []\n",
    "exp_delivery = []\n",
    "availability = []\n",
    "product_url=[]\n",
    "\n",
    "for i in all_url:\n",
    "    driver.get(i)\n",
    "    time.sleep(3)\n",
    "    \n",
    "    try:\n",
    "        brand = driver.find_elements(By.XPATH,'/html/body/div[2]/div/div[5]/div[3]/div[4]/div[47]/div/table/tbody/tr[1]/td[2]/span')\n",
    "        for value in brand:\n",
    "            brand_name.append(value.text)\n",
    "    except NoSuchElementException:\n",
    "        brand_name.append('-')\n",
    "        \n",
    "        \n",
    "    try:\n",
    "        pro_name = driver.find_elements(By.XPATH,'//span[@id=\"productTitle\"]')\n",
    "        for value in pro_name:\n",
    "            product_name.append(value.text)\n",
    "    except NoSuchElementException:\n",
    "        product_name.append('-')\n",
    "        \n",
    "    try:\n",
    "        price_list = driver.find_elements(By.XPATH,'/html/body/div[2]/div/div[5]/div[3]/div[4]/div[13]/div[4]/div[1]/span[2]/span[2]/span[2]')\n",
    "        for value in price_list:\n",
    "            prices.append(value.text)\n",
    "    except NoSuchElementException:\n",
    "        prices.append('-')\n",
    "        \n",
    "    try:\n",
    "        ret_exc = driver.find_elements(By.XPATH,'/html/body/div[2]/div/div[5]/div[3]/div[4]/div[24]/div[2]/div/div/div/div[2]/div/ol/li[3]/div/span/div[2]/span')\n",
    "        for value in ret_exc:\n",
    "            return_exchange.append(value.text)\n",
    "    except NoSuchElementException:\n",
    "        return_exchange.append('-')\n",
    "        \n",
    "    try:\n",
    "        avai = driver.find_elements(By.XPATH,'//div[@id=\"availability\"]/span')\n",
    "        for value in avai:\n",
    "            availability.append(value.text)\n",
    "    except NoSuchElementException:\n",
    "        availability.append('-')\n",
    "        \n",
    "        \n",
    "    try:\n",
    "        exp_del = driver.find_elements(By.XPATH,'//div[@id=\"availability\"]/span')\n",
    "        for value in exp_del:\n",
    "            exp_delivery.append(value.text)\n",
    "    except NoSuchElementException:\n",
    "        exp_delivery.append('-')\n",
    "        \n",
    "        \n",
    "          \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4932e1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(brand_name),len(product_name),len(prices),len(return_exchange),len(exp_delivery),len(availability)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd2591b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "guitar = pd.DataFrame({})\n",
    "guitar['Brand Name'] = brand_name\n",
    "guitar['Name of the Product'] = product_name\n",
    "\n",
    "\n",
    "guitar['Price'] = prices\n",
    "guitar['Return/Exchange'] = return_exchange\n",
    "guitar['Expected Delivery'] = exp_delivery\n",
    "guitar['Availability'] = availability\n",
    "\n",
    "guitar['Product URL'] = all_url\n",
    "guitar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab30840",
   "metadata": {},
   "outputs": [],
   "source": [
    "guitar.to_csv(\"Guitar.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd0b4858",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "450929fd",
   "metadata": {},
   "source": [
    "3. Write a python program to access the search bar and search button on images.google.com and scrape 10\n",
    "images each for keywords ‘fruits’, ‘cars’ and ‘Machine Learning’, ‘Guitar’, ‘Cakes’."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d25d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome()\n",
    "img_urls=[]\n",
    "All_urls=[]\n",
    "url=\"https://images.google.com\"\n",
    "\n",
    "search_items=[\"fruits\", \"cars\",\"Machine Learning\", \"Guitar\", \"Cakes\"]\n",
    "\n",
    "for i in search_items:\n",
    "    driver.get(url)\n",
    "    time.sleep(5)\n",
    "    \n",
    "    search_img = driver.find_element(By.XPATH,'//textarea[@class=\"gLFyf\"]')\n",
    "    search_img.send_keys(str(i))\n",
    "   \n",
    "    search = driver.find_element(By.XPATH,'//span[@class=\"z1asCe MZy1Rb\"]')\n",
    "    search.click()\n",
    "    time.sleep(2)\n",
    "\n",
    "    for _ in range(20):\n",
    "        driver.execute_script(\"window.scrollBy(0,100)\")\n",
    "        imgs= driver.find_elements(By.XPATH,'//img[@class=\"rg_i Q4LuWd\"]')\n",
    "        \n",
    "    \n",
    "    \n",
    "    for images in imgs[0:10]:\n",
    "        source = images.get_attribute('src')\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "source               \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "354d567a",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome()\n",
    "url = \"http://images.google.com/\"\n",
    "\n",
    "# creating empty list\n",
    "urls = []\n",
    "data = []\n",
    "\n",
    "search_item = [\"Fruits\",\"Cars\",\"Machine Learning\"]\n",
    "for i in search_item:\n",
    "    driver.get(url)\n",
    "    time.sleep(5)\n",
    "    \n",
    "    # finding webelement for search_bar\n",
    "    search_img = driver.find_element(By.XPATH,'//textarea[@class=\"gLFyf\"]')\n",
    "    search_img.send_keys(str(i))\n",
    "    time.sleep(5)\n",
    "    search = driver.find_element(By.XPATH,'//button[@class=\"Tg7LZd\"]')\n",
    "    search.click()\n",
    "    \n",
    "    # scroling down the webpage to get some more images\n",
    "    for _ in range(5):\n",
    "        driver.execute_script(\"window.scrollBy(0,10)\")\n",
    "        \n",
    "        imgs = driver.find_elements(By.XPATH,'//img[@class=\"rg_i Q4LuWd\"]')\n",
    "    img_url = []\n",
    "    for image in imgs:\n",
    "        source = image.get_attribute('src')\n",
    "        if source is not None:\n",
    "            if(source[0:4] == 'http'):\n",
    "                img_url.append(source)\n",
    "    for i in img_url[:10]:\n",
    "        urls.append(i)\n",
    "        \n",
    "\n",
    "len(urls)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec2292bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa4f744",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "for i in range(len(urls)):\n",
    "    if i>10 :\n",
    "        break\n",
    "    print(\"Downloading {0} of {1} images \".format(i,10))\n",
    "    response=requests.get(urls[i])\n",
    "    file = open(r\"C:\\Users\\vaibh\\Pictures\\img_folder\\img\" +str(i)+ \".jpg\",\"wb\")\n",
    "    file.write(response.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc990a0f",
   "metadata": {},
   "source": [
    "\n",
    "4. Write a python program to search for a smartphone(e.g.: Oneplus Nord, pixel 4A, etc.) on www.flipkart.com\n",
    "and scrape following details for all the search results displayed on 1st page. Details to be scraped: “Brand\n",
    "Name”, “Smartphone name”, “Colour”, “RAM”, “Storage(ROM)”, “Primary Camera”,\n",
    "“Secondary Camera”, “Display Size”, “Battery Capacity”, “Price”, “Product URL”. Incase if any of the\n",
    "details is missing then replace it by “- “. Save your results in a dataframe and CSV. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf5b428",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome()\n",
    "driver.get(\"https://www.flipkart.com\")\n",
    "\n",
    "# creating empty list\n",
    "all_url = []\n",
    "data = []\n",
    "\n",
    "\n",
    "product_search = driver.find_element(By.XPATH,'//input[@class=\"Pke_EE\"]')\n",
    "product_search.send_keys(\"Oneplus Nord\")\n",
    "time.sleep(2)    \n",
    "search = driver.find_element(By.XPATH,'//button[@class=\"_2iLD__\"]')\n",
    "search.click()\n",
    "\n",
    "url= driver.find_elements(By.XPATH,'//a[@class=\"_1fQZEK\"]')\n",
    "for i in url:\n",
    "     all_url.append(i.get_attribute('href'))\n",
    "    \n",
    "\n",
    "    \n",
    "len(all_url)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4861be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "brand_name = []\n",
    "Smartphone_name=[]\n",
    "color=[]\n",
    "RAM=[]\n",
    "storage=[]\n",
    "primary_camera=[]\n",
    "secondary_camera=[]\n",
    "display_size=[]\n",
    "battery_capacity=[]\n",
    "price=[]\n",
    "url=[]\n",
    "\n",
    "for i in all_url:\n",
    "    driver.get(i)\n",
    "    time.sleep(3)\n",
    "    \n",
    "    url.append(i)\n",
    "    \n",
    "    try:\n",
    "        brand_data = driver.find_elements(By.XPATH,'//span[@class=\"B_NuCI\"]')\n",
    "        for value in brand_data:\n",
    "            brand_name.append(value.text.split()[0])\n",
    "    except NoSuchElementException:\n",
    "        brand_name.append('-')\n",
    "        \n",
    "    try:\n",
    "        Smartphone_data = driver.find_elements(By.XPATH,'/html/body/div[1]/div/div[3]/div[1]/div[2]/div[8]/div[4]/div/div[2]/div/div[1]/table/tbody/tr[3]/td[2]/ul/li')\n",
    "        for value in Smartphone_data:\n",
    "            Smartphone_name.append(value.text)\n",
    "    except NoSuchElementException:\n",
    "        Smartphone_name.append('-')\n",
    "    \n",
    "    try:\n",
    "        color_data = driver.find_elements(By.XPATH,'/html/body/div[1]/div/div[3]/div[1]/div[2]/div[8]/div[4]/div/div[2]/div[1]/div[1]/table/tbody/tr[4]/td[2]/ul/li')\n",
    "        for value in color_data:\n",
    "            color.append(value.text)\n",
    "    except NoSuchElementException:\n",
    "        color.append('-')\n",
    "        \n",
    "    #click on read more \n",
    "    read_more = driver.find_element(By.XPATH,\"//button[@class='_2KpZ6l _1FH0tX']\")\n",
    "    read_more.click()\n",
    "        \n",
    "    try:\n",
    "        RAM_data = driver.find_elements(By.XPATH,'/html/body/div[1]/div/div[3]/div[1]/div[2]/div[8]/div[4]/div/div[2]/div[1]/div[4]/table/tbody/tr[2]/td[2]/ul/li')\n",
    "        for value in RAM_data:\n",
    "            RAM.append(value.text)\n",
    "    except NoSuchElementException:\n",
    "        RAM.append('-')\n",
    "    \n",
    "    \n",
    "        \n",
    "   \n",
    "        \n",
    "    try:\n",
    "        storage_data = driver.find_elements(By.XPATH,'/html/body/div[1]/div/div[3]/div[1]/div[2]/div[8]/div[4]/div/div[2]/div[1]/div[4]/table/tbody/tr[1]/td[2]/ul/li')\n",
    "        for value in storage_data:\n",
    "            storage.append(value.text)\n",
    "    except NoSuchElementException:\n",
    "        storage.append('-')\n",
    "        \n",
    "    \n",
    "        \n",
    "    try:\n",
    "        primary_camera_data = driver.find_elements(By.XPATH,'/html/body/div[1]/div/div[3]/div[1]/div[2]/div[8]/div[4]/div/div[2]/div[1]/div[5]/table/tbody/tr[1]/td[2]/ul/li')\n",
    "        for value in storage_data:\n",
    "            primary_camera.append(value.text)\n",
    "    except NoSuchElementException:\n",
    "        primary_camera.append('-')\n",
    "        \n",
    "    try:\n",
    "        secondary_camera_data = driver.find_elements(By.XPATH,'/html/body/div[1]/div/div[3]/div[1]/div[2]/div[8]/div[4]/div/div[2]/div[1]/div[5]/table/tbody/tr[2]/td[2]/ul/li')\n",
    "        for value in secondary_camera_data:\n",
    "            secondary_camera.append(value.text)\n",
    "    except NoSuchElementException:\n",
    "        secondary_camera.append('-')\n",
    "        \n",
    "    try:\n",
    "        display_size_data = driver.find_elements(By.XPATH,'/html/body/div[1]/div/div[3]/div[1]/div[2]/div[8]/div[4]/div/div[2]/div[1]/div[2]/table/tbody/tr[1]/td[2]/ul/li')\n",
    "        for value in secondary_camera_data:\n",
    "            display_size.append(value.text)\n",
    "    except NoSuchElementException:\n",
    "        display_size.append('-')\n",
    "        \n",
    "    try:\n",
    "        battery_capacity_data = driver.find_elements(By.XPATH,'/html/body/div[1]/div/div[3]/div[1]/div[2]/div[8]/div[4]/div/div[2]/div[1]/div[7]/table/tbody/tr/td[2]/ul/li')\n",
    "        for value in secondary_camera_data:\n",
    "            battery_capacity.append(value.text)\n",
    "    except NoSuchElementException:\n",
    "        battery_capacity.append('-')\n",
    "\n",
    "    try:\n",
    "        price_data = driver.find_elements(By.XPATH,'//div[@class=\"_30jeq3 _16Jk6d\"]')\n",
    "        for value in secondary_camera_data:\n",
    "            price.append(value.text)\n",
    "    except NoSuchElementException:\n",
    "        price.append('-')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f52c2134",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(brand_name),\n",
    "len(Smartphone_name),\n",
    "len(color),\n",
    "len(RAM),\n",
    "len(storage),\n",
    "len(primary_camera),\n",
    "len(secondary_camera),\n",
    "len(display_size),\n",
    "len(battery_capacity),\n",
    "len(price),\n",
    "len(url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee403fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame({'Name':brand_name,'Phone Name':Smartphone_name,'Color':color,'RAM':RAM,'storage':storage,'primary_camera':primary_camera,'secondary_camera':secondary_camera,'display_size':display_size,'battery_capacity':battery_capacity,'price':price,'url':url})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c4c4b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"mobile_data.csv\")\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db84657",
   "metadata": {},
   "source": [
    "5. Write a program to scrap geospatial coordinates (latitude, longitude) of a city searched on google maps. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8229fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome()\n",
    "driver.get(\"https://www.google.com/maps\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc41ba37",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "product_search = driver.find_element(By.XPATH,'//input[@class=\"searchboxinput xiQnY\"]')\n",
    "product_search.send_keys(\"New Delhi\")\n",
    "time.sleep(2)    \n",
    "search = driver.find_element(By.XPATH,'//button[@class=\"mL3xi google-symbols\"]')\n",
    "search.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a834f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "latitude=[]\n",
    "longitude=[]\n",
    "\n",
    "url_string=driver.current_url\n",
    "print(\"Url Extracted :  \",url_string)\n",
    "latitude_longitude=re.findall(r'@(.*)data',url_string)\n",
    "if len(latitude_longitude):\n",
    "    lat_lan_list=latitude_longitude[0].split(\",\")\n",
    "    if(len(lat_lan_list)>=2):\n",
    "        latitude = lat_lng_list[0]\n",
    "        longitude = lat_lng_list[1]\n",
    "        \n",
    "lat_lan_list    \n",
    "print(f\"latitude={latitude} and longitude={longitude}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f5cc8f",
   "metadata": {},
   "source": [
    "6. Write a program to scrap all the available details of best gaming laptops from digit.in. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79848c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome()\n",
    "driver.get(\"https://www.digit.in\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a2c6463",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_10_link = driver.find_element(By.XPATH,'/html/body/div[1]/header/div/div[2]/div/nav/ul/li[3]/a')\n",
    "top_10_link.click()\n",
    "time.sleep(5) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ccba9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "search = driver.find_element(By.XPATH,'/html/body/div[1]/div[3]/div/div[2]/div[1]/div[2]/div[6]/p/a')\n",
    "search.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77166e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "laptop_name=[]\n",
    "operating_system=[]\n",
    "\n",
    "\n",
    "try:\n",
    "    laptop_name_data = driver.find_elements(By.XPATH,'//h3[@class=\"font130 mt0 mb10 mobilesblockdisplay \"]')\n",
    "    for value in laptop_name_data:\n",
    "        laptop_name.append(value.text)\n",
    "except NoSuchElementException:\n",
    "    laptop_name.append('-') \n",
    "    \n",
    "try:\n",
    "    operating_system_data = driver.find_elements(By.XPATH,'/html/body/div[1]/div[3]/div/div/article/div[6]/div/div[2]/div[1]/div/div/div/div[2]/div[3]/div[2]/div[1]/div/span[2]')\n",
    "    for value in operating_system_data:\n",
    "        operating_system.append(value.text)\n",
    "except NoSuchElementException:\n",
    "    operating_system.append('-') \n",
    "    \n",
    "             \n",
    "            \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83749a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df=pd.DataFrame({'Name':laptop_name})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b00b00b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"laptop_data.csv\")\n",
    "driver.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e8fea55",
   "metadata": {},
   "source": [
    "7. Write a python program to scrape the details for all billionaires from www.forbes.com. Details to be scrapped:\n",
    "“Rank”, “Name”, “Net worth”, “Age”, “Citizenship”, “Source”, “Industry”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f27654",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome()\n",
    "driver.get(\"https://www.forbes.com\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beed8cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "menu_link = driver.find_element(By.XPATH,'//div[@class=\"_69hVhdY4\"]')\n",
    "menu_link.click()\n",
    "time.sleep(5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b349fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_data = driver.find_element(By.XPATH,'/html/body/div[1]/div/div[1]/header/div[1]/div[1]/div')\n",
    "list_data.click()\n",
    "time.sleep(2) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d6f8bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_data2 = driver.find_element(By.XPATH,'/html/body/div[1]/div/div[1]/header/div[1]/div[1]/div[2]/ul/li[2]/div[1]')\n",
    "list_data2.click()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa906f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = driver.find_element(By.XPATH,'/html/body/div[1]/div/div[1]/header/div[1]/div[1]/div[2]/ul/li[2]/div[2]/div[3]/ul/li[1]/a')\n",
    "all_data.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dcb2bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "rank=[]\n",
    "name=[]\n",
    "net_worth=[]\n",
    "age=[]\n",
    "citizenship=[]\n",
    "source=[]\n",
    "industry=[]\n",
    "\n",
    "\n",
    "try:\n",
    "    rank_data = driver.find_elements(By.XPATH,'//div[@class=\"Table_rank___YBhk Table_dataCell__2QCve\"]')\n",
    "    for value in rank_data[0:200]:\n",
    "        rank.append(value.text)\n",
    "except NoSuchElementException:\n",
    "    rank.append('-') \n",
    "    \n",
    "    \n",
    "try:\n",
    "    name_data = driver.find_elements(By.XPATH,'//div[@class=\"Table_dataCell__2QCve\"]')\n",
    "    for value in name_data[0:200]:\n",
    "        name.append(value.text)\n",
    "except NoSuchElementException:\n",
    "    name.append('-') \n",
    "    \n",
    "try:\n",
    "    net_worth_data = driver.find_elements(By.XPATH,'//div[@class=\"Table_netWorth___L4R5 Table_dataCell__2QCve\"]')\n",
    "    for value in net_worth_data[0:200]:\n",
    "        net_worth.append(value.text)\n",
    "except NoSuchElementException:\n",
    "    net_worth.append('-') \n",
    "    \n",
    "try:\n",
    "    age_data = driver.find_elements(By.XPATH,'//div[@class=\"Table_dataCell__2QCve\"]')\n",
    "    for value in age_data[0:200]:\n",
    "        age.append(value.text)\n",
    "except NoSuchElementException:\n",
    "    age.append('-') \n",
    "    \n",
    "try:\n",
    "    citizenship_data = driver.find_elements(By.XPATH,'//div[@class=\"TableRow_cell__db-hv Table_cell__houv9\"]')\n",
    "    for value in citizenship_data[0:200]:\n",
    "        citizenship.append(value.text)\n",
    "except NoSuchElementException:\n",
    "    citizenship.append('-') \n",
    "    \n",
    "try:\n",
    "    source_data = driver.find_elements(By.XPATH,'//div[@class=\"Table_dataCell__2QCve\"]/span')\n",
    "    for value in source_data[0:200]:\n",
    "        source.append(value.text)\n",
    "except NoSuchElementException:\n",
    "    source.append('-') \n",
    "    \n",
    "try:\n",
    "    industry_data = driver.find_elements(By.XPATH,'//div[@class=\"Table_dataCell__2QCve\"]/span')\n",
    "    for value in industry_data[0:200]:\n",
    "        industry.append(value.text)\n",
    "except NoSuchElementException:\n",
    "    industry.append('-') \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9bf544b",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(rank),len(name),len(net_worth),len(age),len(citizenship),len(source)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d0795cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame({'Rank':rank,'Name':name,'Net Worth':net_worth,'Age':age,'Citizanship':citizenship,'Source':source}) \n",
    "df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b54534e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"billionaires.csv\")\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c322f6",
   "metadata": {},
   "source": [
    "8. Write a program to extract at least 500 Comments, Comment upvote and time when comment was posted\n",
    "from any YouTube Video. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c87ad514",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome()\n",
    "driver.get(\"https://www.youtube.com\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a9378c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "search = driver.find_element(By.XPATH,'//div[@class=\"ytd-searchbox-spt\"]/input')\n",
    "search.send_keys(\"Animals of Amazon 4K\")   \n",
    "time.sleep(5) \n",
    "search = driver.find_element(By.XPATH,'//button[@class=\"style-scope ytd-searchbox\"]')\n",
    "search.click()\n",
    "#click on first video\n",
    "time.sleep(5)\n",
    "search = driver.find_element(By.XPATH,'//yt-formatted-string[@class=\"style-scope ytd-video-renderer\"]')\n",
    "search.click()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db1d22ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(1000):\n",
    "    driver.execute_script(\"window.scrollBy(0,10000)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3012211c",
   "metadata": {},
   "outputs": [],
   "source": [
    "comments = []\n",
    "comment_time = []\n",
    "likes=[]\n",
    "\n",
    "\n",
    "\n",
    "comments_data = driver.find_elements(By.XPATH,'//span[@class=\"style-scope yt-formatted-string\"]')\n",
    "for value in comments_data[0:501]:\n",
    "    comments.append(value.text)\n",
    "    \n",
    "comment_time_data = driver.find_elements(By.XPATH,'//a[@class=\"yt-simple-endpoint style-scope yt-formatted-string\"]')\n",
    "for value in comment_time_data[0:501]:\n",
    "    comment_time.append(value.text)\n",
    "    \n",
    "likes_data = driver.find_elements(By.XPATH,'//span[@class=\"style-scope ytd-comment-action-buttons-renderer\"]')\n",
    "for value in likes_data[0:501]:\n",
    "    likes.append(value.text)\n",
    "    \n",
    "likes    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cdec6ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(comments),len(comment_time),len(likes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e05219",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame({'comments':comments,'comment_time':comment_time,'likes':likes}) \n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4560f1fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6836572",
   "metadata": {},
   "source": [
    "9. Write a python program to scrape a data for all available Hostels from https://www.hostelworld.com/ in\n",
    "“London” location. You have to scrape hostel name, distance from city centre, ratings, total reviews, overall\n",
    "reviews, privates from price, dorms from price, facilities and property description. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "981fb3b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome()\n",
    "driver.get(\"https://www.hostelworld.com/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e2abffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "search = driver.find_element(By.XPATH,'//div[@class=\"input-wrapper\"]/input')\n",
    "search.send_keys(\"London\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd7ed52",
   "metadata": {},
   "outputs": [],
   "source": [
    "london_search = driver.find_element(By.XPATH,'//ul[@class=\"select-list\"]//li[2]')\n",
    "london_search.click()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca3d759",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_button=driver.find_element(By.XPATH,'//button[@class=\"btn-content medium-button icon-only\"]')\n",
    "search_button.click()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3029a63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "hotel_name = []\n",
    "ratings = []\n",
    "likes=[]\n",
    "distance=[]\n",
    "overall_reviews=[]\n",
    "total_reviews=[]\n",
    "#privates_price=[]\n",
    "#dorms_price=[]\n",
    "\n",
    "\n",
    "hotel_name_data = driver.find_elements(By.XPATH,'//div[@class=\"property-name\"]/span')\n",
    "for value in hotel_name_data:\n",
    "    hotel_name.append(value.text)\n",
    "\n",
    "\n",
    "ratings_data = driver.find_elements(By.XPATH,'//span[@class=\"number\"]')\n",
    "for value in ratings_data:\n",
    "    ratings.append(value.text) \n",
    "    \n",
    "distance_data = driver.find_elements(By.XPATH,'//span[@class=\"distance-description\"]')\n",
    "for value in distance_data:\n",
    "    distance.append(value.text) \n",
    "    \n",
    "distance_data = driver.find_elements(By.XPATH,'//span[@class=\"distance-description\"]')\n",
    "for value in distance_data:\n",
    "    distance.append(value.text) \n",
    "    \n",
    "overall_reviews_data = driver.find_elements(By.XPATH,'//span[@class=\"keyword\"]')\n",
    "for value in overall_reviews_data:\n",
    "    overall_reviews.append(value.text)\n",
    "    \n",
    "total_reviews_data = driver.find_elements(By.XPATH,'//div[@class=\"review\"]/span')\n",
    "for value in total_reviews_data:\n",
    "    total_reviews.append(value.text)\n",
    "    \n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ffefcf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame({'hotel_name':hotel_name,'ratings':ratings,'likes':likes,'distance':distance,'overall_reviews':overall_reviews,'total_reviews':total_reviews}) \n",
    "df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9133f04e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"hostel_data.csv\")\n",
    "driver.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
